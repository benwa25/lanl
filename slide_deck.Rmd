---
title: "Mo' Data Mo' Problems"
subtitle: "Leveraging Data Analytics to Mine Through a Sea of Data - Converge 2019"
output: 
    ioslides_presentation:
        widescreen: true
        incremental: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE, message=FALSE, error=FALSE, warning=FALSE, comment='')
```

```{r pkgs, echo=F, eval=T}
suppressPackageStartupMessages(library(tidyverse))
```

# Agenda

## Agenda

08:00 - 08:30: Welcoming remarks, environment tests, Github profile creation  
08:30 - 08:35: Introduction of me, you, and our expectations  
08:35 - 08:40: Brief introduction to the data science workflow  
08:40 - 08:55: Review of the data engineering that went into this  
08:55 - 09:00: Schedule buffer, bathroom break, and coffee time  
09:00 - 09:30: Introduction to SQL and R through exploratory data analysis  
09:30 - 10:45: Analytical challenges to bolster hypothesis/question generation  
10:45 - 11:00: Schedule buffer, bathroom break, and coffee time  
11:00 - 11:45: Free play  
11:45 - 12:00: Closing remarks and saving to Github  

# Preamble

## Who am I?

- Economist  
- Data scientist  
- Data engineer  
- Data generalist  

## Who are You?

- Student  
- SOC Analyst  
- Threat Hunter  
- Incident Responder  
- Data enthusiast  

## What are we Doing? {.build}

- Learning about data engineering  
- Growing our data science skill set  
- Getting comfortable working with large amounts of data  
- Learning how to use Bash, Git, SQL and R  
- Becoming better analysts  

## What aren't we Doing? {.build}

- Threat hunting  
- Mastering anything  
- Learning machine learning or AI 
- Coming up with any ground breaking insights
- Leaving here without feeling accomplished

##

![](workflow.png)

# Import

## Downloading the Data

```{bash}
# download, unzip, delete compressed file
declare -a files=('auth.txt' 'proc.txt' 'flows.txt' 'dns.txt' 'redteam.txt')
for f in "${files[@]}"
do
    if [ ! -f $f ]
    then
        fname="https://csr.lanl.gov/data/cyber1/$f.gz"
        wget $fname -q --show-progress --progress=bar:force:noscroll
        gunzip $f.gz
    fi
done
```

## Normalizing the Data: Reference Tables

```{bash}
# extract the relevant data to temp files (pseudo code for readability)
awk -F ',' '{print $2 >> "user_domains.txt"} {print $3 >> "user_domains.txt"} 
            {print $4 >> "computers.txt"} {print $5 >> "computers.txt"} 
            {print $6 >> "auth_type.txt"} {print $7 >> "logon_type.txt"}
            {print $8 >> "auth_orientation.txt"}' auth.txt
            
awk -F ',' '{print $3 >> "computers.txt"} {print $4 "ports.txt"}
            {print $5 >> "computers.txt"} {print $6 "ports.txt"}' flows.txt

awk -F ',' '{print $2 >> "user_domains.txt"} {print $3 >> "computers.txt"}
            {print $4 >> processes.txt"}' proc.txt
```

## Normalizing the Data: Reference Tables (cont.)

```{bash}
# extract the relevant data to temp files (pseudo code for readability)
awk -F ',' '{print $2 >> "computers.txt"} {print $3 >> "computers_txt"}' dns.txt

awk -F ',' '{print $2 >> "user_domains.txt"} {print $3 >> "computers.txt"}
            {print $4 >> "computers.txt"}' redteam.txt
            
# dedupe the lookup tables and append row numbers
declare -a files=('computers', 'user_domains', 'ports', 'processes' \
'auth_type' 'auth_orientation' 'logon_type')
for f in "${files[@]}"
do
    cat $f.txt | sort -u | awk '{printf "%s,$s\n",$NR,$0}' >> $f.csv
    rm $f.txt
done
```

## Normalizing the Data: Data Replacement

```{r}
df = read_csv(f, col_names=c('time', 'src_user_domain', 'dest_user_domain',
                             'src_comp', 'dest_comp', 'auth_type', 'logon_type',
                             'auth_orientation', 'success'))
df = df %>%
            left_join(user_domains, by=c('src_user_domain'='name')) %>%
            select(-src_user_domain) %>%
            rename(src_user_domain=id) %>%
            # [truncated]
            left_join(auth_orientation, by=c('auth_orientation'='name')) %>%
            select(-auth_orientation) %>%
            rename(auth_orientation=id) %>%
            mutate(success = as.integer(success == 'Success'))
```

## Storing the Data

```{sql}
-- create lookup table
CREATE TABLE auth_orientation(id int PRIMARY KEY, name varchar(15) NOT NULL);

-- create main table
CREATE TABLE flows(id serial, time int, duration int, protocol int, 
                   packet_count bigint, byte_count bigint, src_comp int, 
                   dest_comp int, src_port int, dest_port int,
                   FOREIGN KEY (src_comp) REFERENCES computers(id),
                   FOREIGN KEY (dest_comp) REFERENCES computers(id),
                   FOREIGN KEY (src_port) REFERENCES ports(id),
                   FOREIGN KEY (dest_port) REFERENCES ports(id));
                   
-- create an index
CREATE INDEX idx_flows_src_comp ON flows(src_comp);

-- copy CSV to table
COPY dns(time, src_comp, rslvd_comp) FROM "./data/dns.csv"
    DELIMITER ',' CSV HEADER;
```

# Tidy

## Label Cleaning

```{sql}
SELECT * FROM auth_type WHERE name LIKE 'MICROSOFT%';

 id |                 name                  
----+---------------------------------------
  5 | MICROSOFT_AUTHENTICA
  6 | MICROSOFT_AUTHENTICAT
  7 | MICROSOFT_AUTHENTICATI
  8 | MICROSOFT_AUTHENTICATIO
  9 | MICROSOFT_AUTHENTICATION
    [TRUNCATED]
 17 | MICROSOFT_AUTHENTICATION_PACKAGE
 18 | MICROSOFT_AUTHENTICATION_PACKAGE_
 19 | MICROSOFT_AUTHENTICATION_PACKAGE_V
 20 | MICROSOFT_AUTHENTICATION_PACKAGE_V1
 21 | MICROSOFT_AUTHENTICATION_PACKAGE_V1_
 22 | MICROSOFT_AUTHENTICATION_PACKAGE_V1_0
```

## Label Cleaning (cont)

```{sql}
UPDATE auths 
SET auth_type=9 
WHERE auth_type IN (
    SELECT id FROM auth_type WHERE name LIKE 'MICROSOFT%'
);
```

## Checking Data Integrity

```{sql}
SELECT COUNT(*) 
FROM (
    SELECT DISTINCT * 
    FROM (
        SELECT time, src_comp, dest_comp FROM auths
    ) a 
    INNER JOIN (
        SELECT time, src_comp, dest_comp 
        FROM redteam
    ) b ON a.time = b.time 
        AND a.src_comp = b.src_comp 
        AND a.dest_comp = b.dest_comp
); -- 699 records

SELECT COUNT(*)
FROM (
    SELECT DISTINCT time, src_comp, dest_comp FROM redteam
) a; -- 713 records
```

# Understand

## Exploratory Data Analysis

1) How many tables are there?  
2) Which tables are reference tables?  
3) How many unique computers are on the network?  
4) How many unique users?  
5) Which two computers talk to each other the most?  
    5a) What is wrong with this analysis?
6) What do the values in the `protocol` column mean?  
7) What is the text value (not the normalized value) of the `logon_type` for the first three authorizations (`id` in 1, 2, and 3)?  
8) What server is queried most often in DNS?  
9) For how many days do we have activity?
10) For how many days were the red team active (that we know about)?

## Question Formulation

Given the data that we have, what are some business questions you'd like to be able to answer?

## Turn Questions into Queries

We're going to spend the rest of the workshop answering questions that could potentially come up in the real world, using the data that we have. 

## Challenge: Identify Days of the Week

When working with data, it's not always possible to know exactly what every variable represents, or what the data is supposed to be telling us. Sometimes, you'll be forced to work with incomplete data and will have to fill in some of the pieces yourself. That's no different with the data we have here.

Time is currently represented as the number of seconds since "epoch", where epoch here means the time when the data was collected. This is done as a form of anonymization, but it would be helpful if we were able to identify what day of the week activity was occuring. Our challenge then, is to identify a way in which we could estimate that with some certainty.

## Challenge: Identify the days of the week

```{sql}
SELECT t.day AS day, COUNT(*) as n
FROM auths a
LEFT JOIN time t ON a.time = t.t_second
GROUP BY t.day
```

```{r, echo=F, eval=T}
cnt_by_day = readRDS('cache/auth_by_day.Rdata')
cnt_by_day
```

## Challenge: Identify the days of the week

```{r}
ggplot(cnt_by_day, aes(x = day, y = n / 1e6)) +
    geom_line() +
    geom_point() +
    scale_y_continuous(breaks = seq(10,30), labels = seq(10,30), 
    				   limits = c(10,30), expand = c(0,0)) +
    scale_x_continuous(breaks = seq(1,59, by=2), labels = seq(1,59,by=2), 
    				   limits = c(1,59)) +
    labs(x = "Day", y = "Authorizations (in millions)") +
    theme(panel.background = element_blank(),
          panel.grid.major = element_line(color = 'gray90'),
          axis.line = element_line(color = "black"))
```

## Challenge: Identify the days of the week

```{r, echo=F, eval=T, fig.align="center"}
ggplot(cnt_by_day, aes(x=day, y=n/1e6)) +
    geom_line() +
    geom_point() +
    scale_y_continuous(breaks=seq(10,30), labels=seq(10,30), limits=c(10,30), expand=c(0,0)) +
    scale_x_continuous(breaks=seq(1,59, by=2), labels=seq(1,59,by=2), limits=c(1,59)) +
    labs(x="Day", y="Authorizations (in millions)") +
    theme(panel.background=element_blank(),
          panel.grid.major=element_line(color='gray90'),
          axis.line=element_line(color="black"))
```

## Challenge: What is the distribution of log on and log off times?

Identity and Access Management analysts are all about making sure that only those who are supposed to be accessing an asset are the ones accessing that asset. User behavior analytics is one of the ways in which they can help achieve their goal. Knowing when their users typically log in and log out for work on a regular basis can help establish a baseline, which makes it possible to alert on activity that is taking place outside of what is normal.

With that in mind, let's identify the distribution of each users' average logon and logoff times.

## Challenge: What is the distribution of log on and log off times?

## Challenge: Which dest computer did each src computer talk to the longest?

Let's say that we're network analysts and we're interested in knowing which computers spend the most time connected to each other passing data back and forth. If we find that a computer is connected to a certain computer for a large period of time, but the data transfer is on par with other connections, then it might be the case that something is wrong with the data transfer between those two computers. Similarly, we might look at the inverse and see the computers with the least amount of time connected and compare that to the amount of data flowing through. Perhaps the infrastructure there can be of value to us, or perhaps the anomalous relationship is something worth investigating.

For this instance, we'll only look at identifying the computer each computer on the network spent the most time passing data to. What this means is that we won't be looking at the bi-directional relationship; instead focusing on a one-way transaction. The reason we might be interested in doing this is to analyze network problems.

## Challenge: Which dest computer did each src computer talk to the longest?

```{sql}
SELECT src_comp, dest_comp, tot_duration 
FROM (
    SELECT src_comp, 
           dest_comp, 
           tot_duration, 
           RANK() over (partition BY src_comp 
                        ORDER BY tot_duration DESC) AS rank 
    FROM (
        SELECT src_comp, dest_comp, SUM(duration) AS tot_duration 
        FROM flows 
        GROUP BY src_comp, dest_comp
    ) a
) b 
WHERE rank = 1;
```

## Challenge: Which flows appear anomalous?

Anomaly detection is all the rage in vendor maketing materials these days, whether it's an IPS solution or UBA. When trying to detect anomalies, having a baseline is paramount to success. Imagine you're a DLP analyst, in charge of making sure sensitive data doesn't leave your network. While scanning packets for keywords might be one approach you take, analyzing anomalous network flows might be another.

If a user on your network is suddently transfering large amounts of data, much more than they normally do, this might be signs of data exfil, and you'll want to be alerted on that as soon as possible.

## Challenge: Which computers are the DCs?

This challenge is merely to demonstrate the power of data visualization in its ability to provide answers, even if you don't know the question that you're asking. What type of visualization do you think would be best for using the auth data to identify which computers are domain controllers?



## Free Play

Now that you've gained some experience in using SQL and R, try using those skills to answer the business questions you came up with earlier. 

Or, if you'd rather, try to identify which of the records in the auths data are malicious.

# Communicate

## Congratulations! 

We have just successfully mined through over 400 GB of data. We looked at authorization data to identify which computers were domain controllers, identified anomalous network traffic, familiarized ourselves with the processes running on our network, and even started looking at which servers outside of the LANL network are being communicated with. We used Bash, SQL, and R to get the data, process it, store it in a database, and then use that database to perform some analyses.

You should be proud of that!

## Push to Github

Let's save our notebook on Github so that others can see what we've accomplished. 

```{bash}
git init
git remote add origin https://github.com/$username/converge2019
git add .
git commit -m "My analysis of the LANL data from Converge 2019"
git push origin master
```