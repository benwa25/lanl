---
title: "Mo' Data Mo' Problems"
subtitle: "Threat Hunting the Data Science Way - Converge 2019"
output: 
    ioslides_presentation:
        widescreen: true
        incremental: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE, message=FALSE, error=FALSE, warning=FALSE, comment='')
```

# Before we dive in

## Who am I? {.build}

- Economist
- Data scientist
- Threat intelligence analyst
- Data enthusiast

## Who are you?

- Student
- SOC Analyst
- Threat Hunter
- Management
- Data enthusiast

## What are we doing?

- Security involves lots of data
- Lots of data is hard to mine through
- Data science helps with that
- Most security folks aren't data scientists
- We're going to change that

##

![](workflow.png)

# Import

## Downloading the data

```{bash}
# download, unzip, delete compmressed file
declare -a files=('auth.txt' 'proc.txt' 'flows.txt' 'dns.txt' 'redteam.txt')
for f in "${files[@]}"
do
    if [ ! -f $f ]
    then
        fname="https://csr.lanl.gov/data/cyber1/$f.gz"
        wget $fname -q --show-progress --progress=bar:force:noscroll
        gunzip $f.gz
    fi
done
```

## Normalizing the data: Reference tables

```{bash}
# create the CSV files
declare -f files=('computers.csv' 'user_domains.csv' 'ports.csv' \
'processes.csv' 'auth_types.csv' 'auth_orientation.csv' 'logon_type.csv')
for f in "${files[@]}"
do
    echo "id,name" > $f
done

# extract the relevant data to temp files (pseudo code for readability)
awk -F ',' '{print $2 >> "user_domains.txt"} {print $3 >> "user_domains.txt"} 
            {print $4 >> "computers.txt"} {print $5 >> "computers.txt"} 
            {print $6 >> "auth_type.txt"} {print $7 >> "logon_type.txt"}
            {print $8 >> "auth_orientation.txt"}' auth.txt
            
awk -F ',' '{print $3 >> "computers.txt"} {print $4 "ports.txt"}
            {print $5 >> "computers.txt"} {print $6 "ports.txt"}' flows.txt
            
```

## Normalizing the data: Reference tables (cont.)

```{bash}
# extract the relevant data to temp files (pseudo code for readability)
awk -F ',' '{print $2 >> "user_domains.txt"} {print $3 >> "computers.txt"}
            {print $4 >> processes.txt"}' proc.txt
            
awk -F ',' '{print $2 >> "computers.txt"} {print $3 >> "computers_txt"}' dns.txt

awk -F ',' '{print $2 >> "user_domains.txt"} {print $3 >> "computers.txt"}
            {print $4 >> "computers.txt"}' redteam.txt
```

## Normalizing the data: Reference tables (cont.)

```{bash}
# dedupe the lookup tables and append row numbers
declare -a files=('computers', 'user_domains', 'ports', 'processes' \
'auth_type' 'auth_orientation' 'logon_type')
for f in "${files[@]}"
do
    cat $f.txt | sort -u | awk '{printf "%s,$s\n",$NR,$0}' >> $f.csv
    rm $f.txt
done
```

## Normalizing the data: Data replacement

```{bash}
# replace strings with index numbers
sort -k 2 -t , auth.txt \
| join -1 2 -2 2 - user_domains.csv -t , -o 1.1,2.1,1.3,1.4,1.5,1.6,1.7,1.8,1.9 \
| sort -k 3 -t , \
| join -1 3 -2 2 - user_domains.csv -t , -o 1.1,1.2,2.1,1.4,1.5,1.6,1.7,1.8,1.9 \
| sort -k 4 -t , \
| join -1 4 -2 2 - computers.csv -t , -o 1.1,1.2,1.3,2.1,1.5,1.6,1.7,1.8,1.9 \
| sort -k 5 -t , \
| join -1 5 -2 2 - computers.csv -t , -o 1.1,1.2,1.3,1.4,2.1,1.6,1.7,1.8,1.9 \
| sort -k 6 -t , \
| join -1 6 -2 2 - auth_type.csv -t , -o 1.1,1.2,1.3,1.4,1.5,2.1,1.7,1.8,1.9 \
| sort -k 7 -t , \
| join -1 7 -2 2 - logon_type.csv -t , -o 1.1,1.2,1.3,1.4,1.5,1.6,2.1,1.8,1.9 \
| sort -k 8 -t , \
| join -1 8 -2 2 - auth_orientation.csv -t , -o 1.1,1.2,1.3,1.4,1.5,1.6,1.7,2.1,1.9 \
| sort -k 1 -t , \
| sed -e "s/Success/1/g;s/Failure/0/g" > auth.csv
```

## Storing the data: PostgreSQL installation

## Storing the data: Table creation

## Storing the data: Optimization

## Storing the data: Data access

# Tidy

# Exploratory data analysis

# Hypothesis generation

# Modeling

# Communication